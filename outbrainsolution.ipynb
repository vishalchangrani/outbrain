{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "# pandas\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "# numpy, matplotlib, seaborn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Toggle based on hardware - if low memory laptop set to True else False\n",
    "poor = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(It was earlier observed that Topics of some documents are not known, Cateogires of some documents are not known, Entities of some documents are not known\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents_meta = pd.read_csv('./data/documents_meta.csv',  dtype={\"document_id\": int, \"source_id\": object, \"publisher_id\": object, \"publish_time\": object})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source Id, Publisher Id and Publish time of all documents <b>are</b> known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Init\n",
    "def add_documents_ids(clicks):\n",
    "    events_df = pd.read_csv('./data/events.csv',  dtype={\"display_id\": int, \"uuid\": str, \"document_id\": int, \"timestamp\": int, \"platform\": str, \"geo_location\": str})\n",
    "    events_df = events_df[events_df.platform != '\\\\N']\n",
    "    events_df.platform = events_df.platform.astype(int)\n",
    "    clicks    = pd.merge(clicks, events_df, on='display_id')\n",
    "    del(events_df) # save memory\n",
    "    ## Add target doucment id info from promoted content\n",
    "    clicks = clicks.rename(index=str, columns={\"document_id\": \"source_document_id\"})\n",
    "    promoted_content = pd.read_csv('./data/promoted_content.csv',  dtype={\"ad_id\": int, \"document_id\": int, \"campaign_id\": object, \"advertiser_id\": object})\n",
    "    clicks = pd.merge(clicks, promoted_content, on='ad_id')\n",
    "    del(promoted_content)\n",
    "    return cleanup(clicks)\n",
    "\n",
    "def add_reg_ctr(clicks):\n",
    "    reg_ctr = pd.read_csv('./reg_ctr.csv',dtype={\"ad_id\":int, \"reg_ctr\":float})\n",
    "    clicks = pd.merge(clicks, reg_ctr, how = 'left', on = 'ad_id')\n",
    "    clicks['reg_ctr'].fillna(0, inplace=True)\n",
    "    del(reg_ctr)\n",
    "    return clicks\n",
    "\n",
    "def add_display_size(clicks):\n",
    "    clicks['display_size'] = clicks_train.groupby(['display_id'], sort=False)['ad_id'].transform('count')\n",
    "    return clicks\n",
    "\n",
    "    #Lets not worry about these for now\n",
    "def cleanup(clicks):\n",
    "    clicks.drop('geo_location', axis=1, inplace=True)\n",
    "    clicks.drop('timestamp', axis=1, inplace=True)\n",
    "    clicks.drop('campaign_id', axis=1, inplace=True)\n",
    "    clicks.drop('advertiser_id', axis=1, inplace=True)\n",
    "    clicks.drop('uuid', axis=1, inplace=True)\n",
    "    return clicks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bringing in the features analysed in FeatureAnalysis.pynb earlier..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def confidence_max(topics):\n",
    "    topics_grouped = topics.groupby(['document_id'], sort=False).agg({'confidence_level':'max'})\n",
    "    topics_grouped = topics_grouped.reset_index()\n",
    "    topics_grouped = topics_grouped.rename(columns={'confidence_level':'confidence_max'})\n",
    "    topics = pd.merge(topics, topics_grouped, how='left', on=['document_id'])\n",
    "    del(topics_grouped)\n",
    "    topics = topics[topics['confidence_level'] == topics['confidence_max']]\n",
    "    topics.drop('confidence_level', axis=1, inplace=True)\n",
    "    topics.drop('confidence_max', axis=1, inplace=True)\n",
    "    topics = topics.drop_duplicates(subset=['document_id'])\n",
    "    return topics\n",
    "def most_frequent(topics, groupbykey, topn):\n",
    "    top_topics = topics.groupby(groupbykey, sort=False)['document_id'].count().sort_values(ascending=False).head(topn).index\n",
    "    return topics[topics[groupbykey].isin(top_topics)]\n",
    "def convert_to_dummies(topics, key):\n",
    "    topics = pd.concat([topics, pd.get_dummies(topics[key])], axis=1, join='inner')\n",
    "    topics.drop(key, axis=1, inplace=True)\n",
    "    #topics = topics.groupby(by='document_id', sort=False).agg(sum).reset_index() #Combine confidence level in one row\n",
    "    return topics\n",
    "def featurize_document_meta(clicks, topics, key, topn):\n",
    "    topics = convert_to_dummies(most_frequent(confidence_max(topics), key, topn), key).to_sparse(fill_value=0);\n",
    "    # Hydrate source document categories/topics/entities\n",
    "    clicks = pd.merge(clicks, topics, how = 'left', left_on = 'source_document_id', right_on = 'document_id')\n",
    "    clicks.drop('document_id_y', axis=1, inplace=True)\n",
    "    clicks.rename(columns={'document_id_x':'document_id'}, inplace=True)\n",
    "    # Hydrate destination document categories/topics/entities\n",
    "    clicks = pd.merge(clicks, topics, how = 'left', left_on = 'document_id', right_on = 'document_id')\n",
    "    clicks.fillna(0, inplace=True) #NaN treated as not belonging to any Category (unknown category)\n",
    "    return clicks  \n",
    "    \n",
    "def featurize(clicks):\n",
    "    categories = pd.read_csv('./data/documents_categories.csv',  dtype={\"document_id\": int, \"category_id\": int, \"confidence_level\": float})\n",
    "    #Create Category dummies\n",
    "    clicks = featurize_document_meta(clicks, categories, 'category_id', 5)\n",
    "    del(categories)\n",
    "    #Create Entity dummies\n",
    "    entities = pd.read_csv('./data/documents_entities.csv',  dtype={\"document_id\": int, \"entity_id\": object, \"confidence_level\": float})\n",
    "    clicks = featurize_document_meta(clicks, entities, 'entity_id', 5)\n",
    "    del(entities)\n",
    "    #Create Topics dummies\n",
    "    topics = pd.read_csv('./data/documents_topics.csv',  dtype={\"document_id\": int, \"topic_id\": int, \"confidence_level\": float})\n",
    "    clicks = featurize_document_meta(clicks, topics, 'topic_id', 5)\n",
    "    del(topics)\n",
    "    clicks.drop('source_document_id', axis=1, inplace=True)\n",
    "    clicks.drop('document_id', axis=1, inplace=True)\n",
    "    return clicks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clicks_train = pd.read_csv('./data/clicks_train.csv',  dtype={\"display_id\": int, \"ad_id\": int, \"clicked\": int})\n",
    "if poor:\n",
    "    clicks_train = clicks_train.head(1001) #1002 is a different display id.\n",
    "clicks_train = add_documents_ids(clicks_train)\n",
    "\n",
    "clicks_train = add_reg_ctr(clicks_train)\n",
    "clicks_train = add_display_size(clicks_train)\n",
    "\n",
    "clicks_train.drop('ad_id', axis=1, inplace=True)\n",
    "clicks_train.drop('display_id', axis=1, inplace=True)\n",
    "\n",
    "clicks_train = featurize(clicks_train)\n",
    "clicks_train = clicks_train.to_sparse(fill_value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from sklearn import tree\n",
    "#model = tree.DecisionTreeClassifier(criterion='gini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model= RandomForestClassifier(n_estimators=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=1000, n_jobs=1, oob_score=False,\n",
       "            random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = clicks_train[clicks_train.columns.difference(['clicked']).values]\n",
    "Y = clicks_train['clicked'].to_dense()\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_split': 1e-07,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 1000,\n",
       " 'n_jobs': 1,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=1000, n_jobs=1, oob_score=False,\n",
       "            random_state=None, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [100, 1000], 'criterion': ['gini', 'entropy']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {   'n_estimators': [100, 1000],\n",
    "                 #'max_depth': [None, 2, 5, 7, 9],\n",
    "                 'criterion': ['gini', 'entropy']\n",
    "             }\n",
    "grid_clf = GridSearchCV(model, param_grid, cv=5)\n",
    "grid_clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy', 'n_estimators': 100}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80119880119880116"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Choose the best model that GridSearch found\n",
    "model = grid_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1045"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del(clicks_train)\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clicks_test = pd.read_csv('./data/clicks_test.csv',  dtype={\"display_id\": int, \"ad_id\": int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clicks_test = add_documents_ids(clicks_test)\n",
    "clicks_test = featurize(clicks_test).to_sparse(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clicks_test['prob'] = model.predict_proba(clicks_test[clicks_test.columns.difference(['display_id', 'ad_id']).values])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clicks_test = clicks_test[['display_id','prob','ad_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clicks_test.sort_values(['display_id', 'prob'], inplace=True, ascending=[True, False])\n",
    "clicks_test.drop('prob', axis=1, inplace=True)\n",
    "clicks_test = clicks_test.groupby(by='display_id', sort=False).aggregate(lambda x: ' '.join([str(ff) for ff in x]))\n",
    "clicks_test.to_csv('submission.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " = predicted\n",
    "\n",
    "probs = model.predict_proba(clicks_test)[:,1]\n",
    "\n",
    "org_train['probs'] = probs\n",
    "\n",
    "org_train.sort_values(['display_id', 'probs'], inplace=True, ascending=[True, False] )\n",
    "\n",
    "Y_ads = org_train[ org_train.clicked == 1 ].ad_id.values.reshape(-1,1)\n",
    "\n",
    "P_ads = org_train.groupby(by='display_id', sort=False).ad_id.apply( lambda x: x.values ).values\n",
    "\n",
    "from ml_metrics import mapk\n",
    "\n",
    "score = mapk( Y_ads, P_ads, 12 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"MAP: %.12f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = org_train.groupby(['display_id']).first()\n",
    "\n",
    "TP = len(result[result['clicked'] == 1])\n",
    "\n",
    "FP = len(result[result['clicked'] != 1])\n",
    "\n",
    "print \"Simple Precision = %.2f\"%(TP / float(TP + FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
